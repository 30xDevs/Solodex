{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ollama' has no attribute 'Chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize the chat object\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChat\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set up a socket server to listen for incom\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ing requests\u001b[39;00m\n\u001b[1;32m     10\u001b[0m server_socket \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39msocket(socket\u001b[38;5;241m.\u001b[39mAF_INET, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ollama' has no attribute 'Chat'"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import socket\n",
    "\n",
    "\n",
    "# Initialize the chat object\n",
    "ollama.GenerateResponse(model=\"llama3.2\", messages=[)\n",
    "chat = ollama.Chat(model=\"llama3.2\")\n",
    "# Set up a socket server to listen for incom\n",
    "# ing requests\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('localhost', 65432))\n",
    "server_socket.listen(1)\n",
    "\n",
    "# response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# print(response['message']['content'])\n",
    "# # or access fields directly from the response object\n",
    "# print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessResponse(models=[Model(model='llama3.2:latest', name='llama3.2:latest', digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', expires_at=datetime.datetime(2025, 1, 31, 21, 33, 32, 983925, tzinfo=TzInfo(-06:00)), size=4030033920, size_vram=4030033920, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
